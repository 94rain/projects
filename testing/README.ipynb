{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "consolidated-madrid",
   "metadata": {
    "papermill": {
     "duration": 0.015326,
     "end_time": "2021-03-10T15:20:54.108602",
     "exception": false,
     "start_time": "2021-03-10T15:20:54.093276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "*Note:* You can run this from your computer (Jupyter or terminal), or use one of the\n",
    "hosted options:\n",
    "\n",
    "[![binder-logo](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ploomber/binder-env/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252Fploomber%252Fprojects%26urlpath%3Dlab%252Ftree%252Fprojects%252Ftesting%252FREADME.ipynb%26branch%3Dmaster)\n",
    "\n",
    "[![deepnote-logo](https://deepnote.com/buttons/launch-in-deepnote-small.svg)](https://deepnote.com/launch?template=deepnote&url=https://github.com/ploomber/projects/blob/master/testing/README.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-attachment",
   "metadata": {
    "papermill": {
     "duration": 0.013541,
     "end_time": "2021-03-10T15:20:54.134965",
     "exception": false,
     "start_time": "2021-03-10T15:20:54.121424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pipeline testing\n",
    "\n",
    "Testing your pipeline is critical to ensure your data expectations hold. When you perform a data transformation, you are expecting the output to have certain properties (e.g. no nulls in certain column). Without testing, these expectations won't be verified and will cause errors errors to propagate to all downstream tasks.\n",
    "\n",
    "These are the most common sources of errors when transforming data:\n",
    "\n",
    "1. A join operation generates duplicated entries because a wrong assumption of a one-to-one relationship (which is really a one-to-many) in the source tables\n",
    "2. A function that aggregates data returns `NULL` because at least one of the input data points was `NULL`\n",
    "3. Dirty data points are used in the analysis (e.g. in a column `age`, you forgot to remove corrupted data points with negative values)\n",
    "\n",
    "Some of these errors are easy to spot (2), but it might take you some tome to find out about others (1 and 3), or worst, you will never notice these errors and just use incorrect data in your analysis. And even if your code is correct and all your expectations hold true, it might not hold true in the future if the data changes and it's important for you to know this as soon as it happens.\n",
    "\n",
    "To make testing effective, **your tests should run every time you run your tasks**. Ploomber has a mechanism to automate this.\n",
    "\n",
    "## Sample data\n",
    "\n",
    "This example loads data from a single table called `my_table`, which has two columns:\n",
    "\n",
    "1. age: ranges from 21 to 80 but there are some corrupted records with -42\n",
    "2. score: ranges from 0 to 10 but there are some corrupted records with missing values\n",
    "\n",
    "Let's take a look at our example `pipeline.yaml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "meaning-sailing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T15:20:54.165015Z",
     "iopub.status.busy": "2021-03-10T15:20:54.164118Z",
     "iopub.status.idle": "2021-03-10T15:20:55.687850Z",
     "shell.execute_reply": "2021-03-10T15:20:55.687267Z"
    },
    "papermill": {
     "duration": 1.540805,
     "end_time": "2021-03-10T15:20:55.688111",
     "exception": false,
     "start_time": "2021-03-10T15:20:54.147306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ploomberutils import display_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "departmental-guitar",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T15:20:55.721048Z",
     "iopub.status.busy": "2021-03-10T15:20:55.720385Z",
     "iopub.status.idle": "2021-03-10T15:20:55.729689Z",
     "shell.execute_reply": "2021-03-10T15:20:55.730168Z"
    },
    "papermill": {
     "duration": 0.030052,
     "end_time": "2021-03-10T15:20:55.730409",
     "exception": false,
     "start_time": "2021-03-10T15:20:55.700357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```yaml\n",
       "clients:\n",
       "  SQLScript: db.get_client\n",
       "  SQLDump: db.get_client\n",
       "\n",
       "tasks:\n",
       "  - source: clean.sql\n",
       "    name: clean\n",
       "    product: ['my_clean_table', 'table']\n",
       "    on_finish: integration_tests.test_sql_clean\n",
       "  \n",
       "  - source: dump.sql\n",
       "    name: dump\n",
       "    class: SQLDump\n",
       "    product: output/my_clean_table.csv\n",
       "    chunksize: null\n",
       "\n",
       "  - source: transform.py\n",
       "    product:\n",
       "        nb: output/transformed.html\n",
       "        data: output/transformed.csv\n",
       "    on_finish: integration_tests.test_py_transform\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_file('pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-virginia",
   "metadata": {
    "papermill": {
     "duration": 0.012331,
     "end_time": "2021-03-10T15:20:55.755745",
     "exception": false,
     "start_time": "2021-03-10T15:20:55.743414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The pipeline has three tasks, one to clean the raw table, another one to dump the clean data to a CSV file and finally, one Python task to transform the data. We included a SQL and a Python task to show how you can test both types of tasks but we recommend you to do as much analysis as you can using SQL because it scales much better than Python code (you won't have to deal with memory errors).\n",
    "\n",
    "The configuration is straightforward, the only new key is `on_finish` (inside the first and third task). This is known as a *hook*. Task hooks allow you to embed custom logic when certain events happen. `on_finish` is executed after a task successfully executes. The value is a dotted path, which tells Ploomber where to find your testing function. Under the hood, Ploomber will import your function and call it after the task is executed, here's some equivalent code:\n",
    "\n",
    "```python\n",
    "from integration_tests import test_sql_clean\n",
    "\n",
    "# your task is executed...\n",
    "\n",
    "# ploomber calls your testing function...\n",
    "test_sql_clean()\n",
    "```\n",
    "\n",
    "Before diving into the testing source code, let's see the rest of the tasks.\n",
    "\n",
    "`clean.sql` just filters columns we don't want to include in the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "automotive-hayes",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T15:20:55.784170Z",
     "iopub.status.busy": "2021-03-10T15:20:55.783385Z",
     "iopub.status.idle": "2021-03-10T15:20:55.787017Z",
     "shell.execute_reply": "2021-03-10T15:20:55.787521Z"
    },
    "papermill": {
     "duration": 0.020048,
     "end_time": "2021-03-10T15:20:55.787747",
     "exception": false,
     "start_time": "2021-03-10T15:20:55.767699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```postgresql\n",
       "DROP TABLE IF EXISTS {{product}};\n",
       "\n",
       "CREATE TABLE {{product}} AS\n",
       "SELECT * FROM my_table\n",
       "WHERE score is not null AND age > 0\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_file('clean.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-table",
   "metadata": {
    "papermill": {
     "duration": 0.01309,
     "end_time": "2021-03-10T15:20:55.813669",
     "exception": false,
     "start_time": "2021-03-10T15:20:55.800579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`dump.sql` just selects all rows from the clean table to dump it to the CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cultural-alabama",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T15:20:55.844454Z",
     "iopub.status.busy": "2021-03-10T15:20:55.843683Z",
     "iopub.status.idle": "2021-03-10T15:20:55.847281Z",
     "shell.execute_reply": "2021-03-10T15:20:55.847856Z"
    },
    "papermill": {
     "duration": 0.02093,
     "end_time": "2021-03-10T15:20:55.848147",
     "exception": false,
     "start_time": "2021-03-10T15:20:55.827217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```postgresql\n",
       "SELECT * FROM {{upstream['clean']}}\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_file('dump.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-shuttle",
   "metadata": {
    "papermill": {
     "duration": 0.012606,
     "end_time": "2021-03-10T15:20:55.873554",
     "exception": false,
     "start_time": "2021-03-10T15:20:55.860948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, the `transform.py` script generates a new column using `score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "complex-tracy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T15:20:55.903318Z",
     "iopub.status.busy": "2021-03-10T15:20:55.902605Z",
     "iopub.status.idle": "2021-03-10T15:20:55.906537Z",
     "shell.execute_reply": "2021-03-10T15:20:55.907042Z"
    },
    "papermill": {
     "duration": 0.020789,
     "end_time": "2021-03-10T15:20:55.907299",
     "exception": false,
     "start_time": "2021-03-10T15:20:55.886510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```py\n",
       "import pandas as pd\n",
       "\n",
       "# + tags=[\"parameters\"]\n",
       "upstream = ['dump']\n",
       "product = None\n",
       "\n",
       "# +\n",
       "df = pd.read_csv(upstream['dump'])\n",
       "df['multiplied_score'] = df.score * 42\n",
       "\n",
       "# +\n",
       "df.to_csv(product['data'])\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_file('transform.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-edwards",
   "metadata": {
    "papermill": {
     "duration": 0.01467,
     "end_time": "2021-03-10T15:20:55.935383",
     "exception": false,
     "start_time": "2021-03-10T15:20:55.920713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's now take a look at our tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incorrect-morgan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T15:20:55.968385Z",
     "iopub.status.busy": "2021-03-10T15:20:55.967501Z",
     "iopub.status.idle": "2021-03-10T15:20:55.971357Z",
     "shell.execute_reply": "2021-03-10T15:20:55.971852Z"
    },
    "papermill": {
     "duration": 0.022995,
     "end_time": "2021-03-10T15:20:55.972089",
     "exception": false,
     "start_time": "2021-03-10T15:20:55.949094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```py\n",
       "import pandas as pd\n",
       "from ploomber.testing.sql import nulls_in_columns, range_in_column\n",
       "\n",
       "\n",
       "def test_sql_clean(client, product):\n",
       "    \"\"\"Tests for clean.sql\n",
       "    \"\"\"\n",
       "    assert not nulls_in_columns(client, ['score', 'age'], product)\n",
       "    min_age, max_age = range_in_column(client, 'age', product)\n",
       "    assert min_age > 0\n",
       "\n",
       "\n",
       "def test_py_transform(product):\n",
       "    \"\"\"Tests for transform.py\n",
       "    \"\"\"\n",
       "    df = pd.read_csv(str(product['data']))\n",
       "    assert not df.multiplied_score.isna().sum()\n",
       "    assert df.multiplied_score.min() >= 0\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_file('integration_tests.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-whole",
   "metadata": {
    "papermill": {
     "duration": 0.013411,
     "end_time": "2021-03-10T15:20:55.999198",
     "exception": false,
     "start_time": "2021-03-10T15:20:55.985787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Testing Python scripts\n",
    "\n",
    "To test your Python scripts, you have to know which file to look at. You can do so by simply adding `product` as argument to your function. If your Python script generates more than one product (like in our case), `product` will be a dictionary-like object, that's why we are using `product['data']`. This returns a `Product` object, to get the path to the file, simply use the `str` function.\n",
    "\n",
    "```pycon\n",
    ">>> product # dictionary-like object: maps names to Product objects\n",
    ">>> product['data'] # Product object\n",
    ">>> str(product['data']) # path to the data file\n",
    "```\n",
    "\n",
    "## Testing SQL scripts\n",
    "\n",
    "To test SQL scripts, you also need the client to send queries to the appropriate database, to do so, just add `client` to your testing function.\n",
    "\n",
    "The `ploomber.testing.sql` module implements convenient functions to test your tables. They always take `client` as its first argument, just pass the client variable directly. Since our SQL script only generates a product, you can directly pass the product object to the testing function (otherwise pass `product[key]`) with the appropriate key.\n",
    "\n",
    "*Note:* If you're implementing your own SQL testing logic, doing `str(product)` will return a `{schema}.{name}` string, you can also use `product.schema` and `product.name`.\n",
    "\n",
    "## Running the pipeline\n",
    "\n",
    "Before we run the pipeline, we generate a sample database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "olive-polymer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T15:20:56.035329Z",
     "iopub.status.busy": "2021-03-10T15:20:56.034548Z",
     "iopub.status.idle": "2021-03-10T15:20:56.620028Z",
     "shell.execute_reply": "2021-03-10T15:20:56.620593Z"
    },
    "papermill": {
     "duration": 0.607777,
     "end_time": "2021-03-10T15:20:56.620918",
     "exception": false,
     "start_time": "2021-03-10T15:20:56.013141",
     "status": "completed"
    },
    "tags": [
     "bash"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: line 1: fg: no job control\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "%%bash\n",
    "cd setup\n",
    "python script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-roller",
   "metadata": {
    "papermill": {
     "duration": 0.014345,
     "end_time": "2021-03-10T15:20:56.649624",
     "exception": false,
     "start_time": "2021-03-10T15:20:56.635279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's now run our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polar-humanitarian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T15:20:56.687624Z",
     "iopub.status.busy": "2021-03-10T15:20:56.686721Z",
     "iopub.status.idle": "2021-03-10T15:21:01.801002Z",
     "shell.execute_reply": "2021-03-10T15:21:01.801646Z"
    },
    "papermill": {
     "duration": 5.137095,
     "end_time": "2021-03-10T15:21:01.801929",
     "exception": false,
     "start_time": "2021-03-10T15:20:56.664834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name       Ran?      Elapsed (s)    Percentage\n",
      "---------  ------  -------------  ------------\n",
      "clean      True         0.013248      0.681447\n",
      "dump       True         0.002425      0.124737\n",
      "transform  True         1.92843      99.1938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building task \"transform\":   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Executing:   0%|          | 0/5 [00:00<?, ?cell/s]\u001b[A\n",
      "Executing: 100%|██████████| 5/5 [00:01<00:00,  3.08cell/s]\n",
      "Building task \"transform\": 100%|██████████| 3/3 [00:01<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ploomber build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-administration",
   "metadata": {
    "papermill": {
     "duration": 0.015033,
     "end_time": "2021-03-10T15:21:01.833941",
     "exception": false,
     "start_time": "2021-03-10T15:21:01.818908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Everything looks good.\n",
    "\n",
    "Let's now imagine a colleague found an error in the cleaning logic and has re-written the script. However, he was unaware that both columns in the raw table had corrupted data and forgot to include the filtering conditions.\n",
    "\n",
    "The script now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cordless-horse",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T15:21:01.870642Z",
     "iopub.status.busy": "2021-03-10T15:21:01.869901Z",
     "iopub.status.idle": "2021-03-10T15:21:01.874260Z",
     "shell.execute_reply": "2021-03-10T15:21:01.874746Z"
    },
    "papermill": {
     "duration": 0.02492,
     "end_time": "2021-03-10T15:21:01.874992",
     "exception": false,
     "start_time": "2021-03-10T15:21:01.850072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```postgresql\n",
       "DROP TABLE IF EXISTS {{product}};\n",
       "\n",
       "CREATE TABLE {{product}} AS\n",
       "SELECT * FROM my_table\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = Path('clean.sql')\n",
    "new_code = path.read_text().replace('WHERE score is not null AND age > 0', '')\n",
    "path.write_text(new_code)\n",
    "display_file('clean.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-norman",
   "metadata": {
    "papermill": {
     "duration": 0.014557,
     "end_time": "2021-03-10T15:21:01.904958",
     "exception": false,
     "start_time": "2021-03-10T15:21:01.890401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's see what happens if we run the pipeline (don't be intimidated by the long traceback, we'll explain it in a bit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bottom-reunion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T15:21:01.942972Z",
     "iopub.status.busy": "2021-03-10T15:21:01.942276Z",
     "iopub.status.idle": "2021-03-10T15:21:05.127373Z",
     "shell.execute_reply": "2021-03-10T15:21:05.127951Z"
    },
    "papermill": {
     "duration": 3.207665,
     "end_time": "2021-03-10T15:21:05.128197",
     "exception": false,
     "start_time": "2021-03-10T15:21:01.920532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture captured\n",
    "%%sh --no-raise-error\n",
    "ploomber build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "changed-upset",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T15:21:05.164963Z",
     "iopub.status.busy": "2021-03-10T15:21:05.164068Z",
     "iopub.status.idle": "2021-03-10T15:21:05.166898Z",
     "shell.execute_reply": "2021-03-10T15:21:05.167440Z"
    },
    "papermill": {
     "duration": 0.023706,
     "end_time": "2021-03-10T15:21:05.167677",
     "exception": false,
     "start_time": "2021-03-10T15:21:05.143971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task \"clean\": 100%|██████████| 3/3 [00:00<00:00, 163.35it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/cli/io.py\", line 20, in wrapper\n",
      "    fn(**kwargs)\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/cli/build.py\", line 38, in main\n",
      "    report = dag.build(force=args.force, debug=args.debug)\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/dag/DAG.py\", line 428, in build\n",
      "    report = callable_()\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/dag/DAG.py\", line 527, in _build\n",
      "    raise build_exception\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/dag/DAG.py\", line 459, in _build\n",
      "    task_reports = self._executor(dag=self,\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/executors/serial.py\", line 135, in __call__\n",
      "    raise DAGBuildError(str(exceptions_all))\n",
      "ploomber.exceptions.DAGBuildError: \n",
      "=============================== DAG build failed ===============================\n",
      "--------- SQLScript: clean -> SQLRelation(('my_clean_table', 'table')) ---------\n",
      "-------------- /Users/Edu/dev/projects-ploomber/testing/clean.sql --------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/tasks/abc.py\", line 505, in _build\n",
      "    self._finish_task_execution()\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/tasks/abc.py\", line 305, in _finish_task_execution\n",
      "    self._run_on_finish()\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/tasks/abc.py\", line 297, in _run_on_finish\n",
      "    self.on_finish(**kwargs)\n",
      "  File \"/Users/Edu/dev/projects-ploomber/testing/integration_tests.py\", line 8, in test_sql_clean\n",
      "    assert not nulls_in_columns(client, ['score', 'age'], product)\n",
      "AssertionError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/executors/serial.py\", line 183, in catch_exceptions\n",
      "    fn()\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/executors/serial.py\", line 156, in __call__\n",
      "    return self.fn(**self.kwargs)\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/executors/serial.py\", line 163, in catch_warnings\n",
      "    result = fn()\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/executors/serial.py\", line 156, in __call__\n",
      "    return self.fn(**self.kwargs)\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/executors/serial.py\", line 232, in build_in_subprocess\n",
      "    report, meta = task._build(**build_kwargs)\n",
      "  File \"/Users/Edu/dev/ploomber/src/ploomber/tasks/abc.py\", line 517, in _build\n",
      "    raise TaskBuildError(msg) from e\n",
      "ploomber.exceptions.TaskBuildError: Exception when running on_finish for task \"clean\": \n",
      "=============================== Summary (1 task) ===============================\n",
      "SQLScript: clean -> SQLRelation(('my_clean_table', 'table'))\n",
      "=============================== DAG build failed ===============================\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(captured.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-baking",
   "metadata": {
    "papermill": {
     "duration": 0.016562,
     "end_time": "2021-03-10T15:21:05.200096",
     "exception": false,
     "start_time": "2021-03-10T15:21:05.183534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ploomber error messages are designed to give you enough context, so you can fix things quickly.\n",
    "\n",
    "The last line says that our pipeline failed to build:\n",
    "\n",
    "```\n",
    "ploomber.exceptions.DAGBuildError: Failed to build DAG\n",
    "```\n",
    "\n",
    "That's a very general error message, but it tells us at which stage our pipeline failed (building is not the only one). If you go up a few lines, you'll see this:\n",
    "\n",
    "```\n",
    "ploomber.exceptions.TaskBuildError: Exception when running on_finish for task \"clean\"\n",
    "```\n",
    "\n",
    "That's a bit more specific. It's pointing us to the `on_finish` hook in the `clean` task. Go up a few more lines:\n",
    "\n",
    "```\n",
    "assert not nulls_in_columns(client, ['score', 'age'], product)\n",
    "AssertionError\n",
    "```\n",
    "\n",
    "That tells me the exact test that failed! While having this long error messages might seem to verbose, it helps a lot to understand why the pipeline failed, our take away from the error message is: \"the pipeline building process failed because the `on_finish` hook in the `clean` task raised an exception in this line\". That's much better than either \"the pipeline failed\" or \"this line raised an exception\".\n",
    "\n",
    "Let's fix our pipeline and add the `WHERE` clause again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "italian-ghost",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T15:21:05.237987Z",
     "iopub.status.busy": "2021-03-10T15:21:05.237241Z",
     "iopub.status.idle": "2021-03-10T15:21:05.241116Z",
     "shell.execute_reply": "2021-03-10T15:21:05.241661Z"
    },
    "papermill": {
     "duration": 0.025379,
     "end_time": "2021-03-10T15:21:05.241900",
     "exception": false,
     "start_time": "2021-03-10T15:21:05.216521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```postgresql\n",
       "DROP TABLE IF EXISTS {{product}};\n",
       "\n",
       "CREATE TABLE {{product}} AS\n",
       "SELECT * FROM my_table\n",
       "WHERE score is not null AND age > 0\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = Path('clean.sql')\n",
    "new_code = path.read_text() + 'WHERE score is not null AND age > 0'\n",
    "path.write_text(new_code)\n",
    "display_file('clean.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "advisory-asset",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T15:21:05.281156Z",
     "iopub.status.busy": "2021-03-10T15:21:05.280403Z",
     "iopub.status.idle": "2021-03-10T15:21:10.366236Z",
     "shell.execute_reply": "2021-03-10T15:21:10.366720Z"
    },
    "papermill": {
     "duration": 5.109491,
     "end_time": "2021-03-10T15:21:10.366930",
     "exception": false,
     "start_time": "2021-03-10T15:21:05.257439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name       Ran?      Elapsed (s)    Percentage\n",
      "---------  ------  -------------  ------------\n",
      "clean      True         0.011989      0.621151\n",
      "dump       True         0.002153      0.111547\n",
      "transform  True         1.91599      99.2673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building task \"transform\":   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Executing:   0%|          | 0/5 [00:00<?, ?cell/s]\u001b[A\n",
      "Executing: 100%|██████████| 5/5 [00:01<00:00,  3.09cell/s]\n",
      "Building task \"transform\": 100%|██████████| 3/3 [00:01<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ploomber build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-advocate",
   "metadata": {
    "papermill": {
     "duration": 0.016977,
     "end_time": "2021-03-10T15:21:10.401415",
     "exception": false,
     "start_time": "2021-03-10T15:21:10.384438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All good! Pipeline is running without issues again!\n",
    "\n",
    "## Test-driven development (TDD)\n",
    "\n",
    "Writing data tests is essential for developing robust pipelines. Coding tests is simple, all we have to do is write in code that we already have in our mind when thinking what the outcome of a script should be.\n",
    "\n",
    "This thought process happens *before* we write the actual code, which means we could easily write tests even before we write the actual code. This approach is called Test-driven development (TDD).\n",
    "\n",
    "Following this framework has an added benefit, since we force ourselves to put in concrete terms our data expectations, it makes easier to think how we want to get there.\n",
    "\n",
    "Furthermore, *tests also serve as documentation* for us (and for others). By looking at our tests, anyone can see what *our intent* is. Then by looking at the code, it will be easier to spot mismatches between our intent and our implementation.\n",
    "\n",
    "\n",
    "## Pro tip: debugging and developing tests interactively\n",
    "\n",
    "Even though tests are usually just a few short statements, writing them in an interactive way can help you quickly prototype your assertions. One simple trick you can use to do this is to start an interactive session and load the `client` and `product` variables.\n",
    "\n",
    "Let's imagine you want to write a test for a new SQL script (but the same applies for other types of scripts). You add a testing function, but it's currently empty:\n",
    "\n",
    "```python\n",
    "def my_sql_testing_function(client, product):\n",
    "    pass\n",
    "```\n",
    "\n",
    "If you run this, Ploomber will still call your function, you can start an interactive session when this happens:\n",
    "\n",
    "```python\n",
    "def my_sql_testing_function(client, product):\n",
    "    from IPython import embed; embed()\n",
    "```\n",
    "\n",
    "Once you call `ploomber build`, wait for the Python prompt to show and verify you have the `client` and `product` variables:\n",
    "\n",
    "```pycon\n",
    ">>> print(client)\n",
    ">>> print(product)\n",
    "```\n",
    "\n",
    "\n",
    "## Where to go next\n",
    "\n",
    "* [Documentation for ploomber.testing - Handy functions for testing pipelines](../api/testing.rst)\n",
    "* [Our blog post on CI for Data Science (which includes a section on testing pipelines)](https://towardsdatascience.com/rethinking-continuous-integration-for-data-science-ebf0dfc61788)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "papermill": {
   "duration": 17.662737,
   "end_time": "2021-03-10T15:21:10.906743",
   "exception": null,
   "input_path": "/var/folders/3h/_lvh_w_x5g30rrjzb_xnn2j80000gq/T/tmp1vfi65qv.ipynb",
   "output_path": "../../projects-ploomber/testing/README.ipynb",
   "parameters": {
    "product": "../../projects-ploomber/testing/README.ipynb"
   },
   "start_time": "2021-03-10T15:20:53.244006"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
