<!-- #region -->
# SQL templating

## Basic templating

SQL templating is a powerful way to make your SQL scripts more concise. It works by embedding a templating language (Ploomber uses [jinja](https://github.com/pallets/jinja)) that uses placeholders that then are rendered into the final SQL code that is sent to the database.

If you've followed the SQL pipelines tutorial in the Get started section, you've already used SQL templating. Let's take a look at a simple script that follows the common structure for SQL pipelines:

```postgresql
{% set product = SQLRelation(['my_schema', 'my_table', 'table']) %}

DROP TABLE IF EXISTS {{product}};

CREATE TABLE {{product}} AS
SELECT * FROM {{upstream['clean']}}
WHERE x > 10
```

The `{% set .. %}` statement defines a variable. This serves two purposes. It tells Ploomber that this script is going to create a `table` named `my_table` in a schema named `my_schema`, this is required so Ploomber can forward this value to downstream SQL scripts and they know which table to use as input. The second purpose is to be a placeholder so we only have to declare the table name once.

When you re-run your pipeline after making some changes, you want to overwrite any existing table with the same name, this is why we need the `DROP TABLE IF EXISTS ...;` statement before we run `CREATE TABLE ..;`, since both statements need the table name, we can just pass the `{{product}}` placeholder.

Finally, the `{{upstream['clean']}}` placeholders tells Ploomber that the current script uses the product from a task named `clean` as input data. This defines the dependency relationship between these two scripts and also implies that the placeholder will be replaced by the actual table/view generated by the `clean` task.

These are the basic elements for templated SQL, but there are many other things we can do. This tutorial explains other useful patterns.

## Control structures

jinja offers control structures that help us write concise SQL code. Say we want to compute summary statistics for a given column:

```postgresql
SELECT
    some_column,
    AVG(another_column) as avg_another_column,
    STDEV(another_column) as stdev_another_column,
    COUNT(another_column) as count_another_column,
    SUM(another_column) as sum_another_column,
    MAX(another_column) as max_another_column,
    MIN(another_column) as min_another_column,
FROM some_table
GROUP BY some_column
```

This code is already very repetitive, now imagine how repetitive it would be if we wanted to compute aggregations for more than one column. We can generate the same code succintly using control structures:

```postgresql
SELECT
    some_column,
-- loop over aggregation functions
{% for fn in ['AVG', 'STDEV', 'COUNT', 'SUM', 'MAX', 'MIN'] %}
    -- apply function to the column, name the column
    -- and only add a comma if we are not in the last loop element
    {{fn}}({{col_agg}}) as {{fn}}_{{col_agg}}{{ ',' if not loop.last else '' }}
{% endfor %}
FROM some_table
GROUP BY some_column
```

<!-- #endregion -->

## Macros

Macros allow us to maximize SQL code reusability by defining snippets that we can "import" in other files. To define a macro we just need to enclose our snippet between the  `{% macro %} ... {% endmacro %}` tags. Let's create a macro out of our previous snippet:


```python
from ploomberutils import display_file
```

```python
display_file('sql/macros.sql', syntax='postgresql')
```

The `{% macro %}` tag defines the macro name and parameters (if any). To use our macro in a different file, we have to import it. Let's say we define the previous macro in a `macros.sql` file:

```python
display_file('sql/create-table.sql', syntax='postgresql')
```

### Configuring support for macros

To work with macros, we have to make a small change to our `pipeline.yaml` file. So far, to tell Ploomber which SQL scripts to use, we've just passed the path to the script in the `source` key. To be able to import macros in our scripts we have to configure a source loader.

A source loader is simply a folder with files, with a small addition: it defines an environment that makes imports work.

Let's say all the scripts in our pipeline are in a `sql/` directory. `sql/` has two scripts, which correspond to the files shown in the previous section.

Our project structure looks like this:

```sh
tree
```

To configure our source loader. We just need to add a `source_loader` section like this:

```python
display_file('pipeline.yaml')
```

## Printing rendered code

Templated SQL is a powerful to write less code. But it might get difficult to debug templating errors that render invalid SQL scripts. Taking a look at the rendered code helps detect templating errors. The command line interface provides a command for doing this:


```sh
ploomber task sql-task --source
```

As we can see, our template is working, and it renders to a valid SQL script. But if it didn't it'd be easier to spot errors in the rendered code than in the templated source.

## Where to go next

* [Jinja documentation](https://jinja.palletsprojects.com/en/2.11.x/templates/)

